\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{4}

\begin{document}




\chapter{Entropy and the Third Law of Thermodynamics}
\section{Entropy and the Third Law}
\begin{itemize}
    \item \marginnote{2/7:}An experimental determination of entropy.
    \begin{itemize}
        \item We have that
        \begin{align*}
            \dd{U} &= \var{q_\text{rev}}+\var{w_\text{rev}}\\
            &= T\dd{S}-P\dd{V}
        \end{align*}
        \item At constant $V$, $\dd{V}=0$, so $\dd{S}=\dd{U}/T=C_V\dd{T}/T$.
        \item More rigorously, we can do the rest of the derivation as in \textcite{bib:McQuarrieSimon} to get
        \begin{align*}
            \left( \pdv{S}{T} \right)_V &= \frac{C_V}{T}&
            \left( \pdv{S}{V} \right)_T &= \frac{1}{T}\left[ P+\left( \pdv{U}{V} \right)_T \right]
        \end{align*}
        \item It follows that $\Delta S=\int_{T_1}^{T_2}C_V/T\dd{T}$.
        \begin{itemize}
            \item This is one way to measure the change in entropy.
            \item However, it's not very practical since it's very hard to do constant volume chemistry.
        \end{itemize}
    \end{itemize}
    \item Derives
    \begin{align*}
        \left( \pdv{S}{T} \right)_P &= \frac{C_P}{T}&
        \left( \pdv{S}{P} \right)_T &= \frac{1}{T}\left[ \left( \pdv{H}{P} \right)-V \right]
    \end{align*}
    as in \textcite{bib:McQuarrieSimon}.
    \begin{itemize}
        \item It follows that $\Delta S=\int_{T_1}^{T_2}C_P/T\dd{T}$, too.
        \item However, since this is $\Delta S$ at constant pressure, we expect it to be bigger than $\Delta S$ at constant volume because work is done.
    \end{itemize}
    \item \textbf{Third Law}: The entropy of pure ordered crystals is zero at \SI{0}{\kelvin}.
    \begin{itemize}
        \item Solid \ce{CO} is \emph{not} ordered, as discussed in \textcite{bib:McQuarrieSimon}.
        \item Derive with $S=k_B\ln W$.
    \end{itemize}
    \item Since we can't actually achieve \SI{0}{\kelvin} and measure properties such as entropy there, we appeal to the Debye model to determine entropy there and in the viscinity.
    \item \textbf{Debye model}: The statement that $C_P(T)\propto T^3=AT^3$ for insulating crystals.
    \begin{itemize}
        \item It follows that for $T_0$ sufficiently close to \SI{0}{\kelvin},
        \begin{equation*}
            \Delta S = \int_0^{T_0}\frac{AT^3}{T}\dd{T}
            = \frac{1}{3}AT_0^3
            = \frac{C_P(T_0)}{3}
        \end{equation*}
        \item Justifying the Debye model.
        \begin{itemize}
            \item According to the Einstein model, each atom is a harmonic oscillator of frequency $h\nu$. This implies, however, that $C_P(T)\approx 1/(\e[h\nu/k_BT]-1)$ drops too fast at low $T$.
            \item Debye says that sound waves are harmonic oscillators with very low frequencies. If you take $h\nu\ll k_BT$, or $hc/\lambda\ll k_BT$, you are concerned with all $\lambda$ sufficiently large (or wavevectors $k=2\pi/\lambda$ sufficiently small). The wavevectors are within a sphere in a wavevector space with basis $k_x,k_y,k_z$. The radius of this sphere is proportional to $T$. Thus, it's volume, which contains what we're interested in, is $4/3\pi r^3$ and thus the sum of all the wavevectors is proportional to $T^3$. Thus, the number of states of energy less than $k_BT$ is proportional to $T^3$ and each has $k_BT$ of energy (thus, if you were trying to find a scaling for the energy, it would be $U(T)\approx T^4$). Note also $h\nu=hc/\lambda=2\pi\hbar c/\lambda=c\hbar k$.
        \end{itemize}
    \end{itemize}
    \item Comparing experimental and theoretical values of entropy of gases to the theory value from the partition function.
    \begin{itemize}
        \item Ideal gas phase $S$ is accurately calculated with $Q=q^N/N!$.
        \item Table 21.4 compares the agreement for seven substances and finds it accurate to within $0.1\%$.
        \item The discrepancies come from disorder at \SI{0}{\kelvin}, as described in \textcite{bib:McQuarrieSimon}.
    \end{itemize}
\end{itemize}



\section{Introduction to Free Energies}
\begin{itemize}
    \item \marginnote{2/9:}Discusses the entropy trends from Chapter 21.
    \begin{itemize}
        \item More atoms ($\ce{CO}<\ce{CO2}$) increases entropy.
        \item Heavier atoms increases entropy.
        \item Floppier molecules ($\text{pentane}>\text{cyclopentane}$) have greater entropy.
        \item Gases have greater entropy (this rule is king).
    \end{itemize}
    \item Example (21-42):
    \begin{itemize}
        \item When determining which chemical reaction has the greatest increase in entropy, look at the change in number of moles of gas as the first tie-breaker.
    \end{itemize}
    \item Consider two systems: Water and ice in a container in thermal contact with a surrounding at zero celsius, and water at room temperature and ice at zero celsius in a thermally isolated container.
    \begin{itemize}
        \item The first process is reversible since we can melt by raising the temperature slightly (allowing heat flow into the system) and vice versa for freezing.
        \item The second process is irreversible because it tends toward thermal equilibrium and there is no way to undo the final equilibrium.
        \item We can consider reversible paths for both processes though to calculate state functions.
        \item Goes over how to calculate the final temperature $T$ in the second process, which is necessary to get the entropy componentwise.
    \end{itemize}
    \item Free energies decrease in a spontaneous process.
    \begin{itemize}
        \item We know that $\dd{S}\geq 0$ for an isolated system, and that $\dd{S}\geq\var{q}/T$ for any system (equality holds for reversible processes).
        \item Rearranging, we have that
        \begin{equation*}
            0 \geq \var{q}-T\dd{S}
        \end{equation*}
        \item It follows since $\dd{U}=\var{q}+\var{w}$ that
        \begin{align*}
            \dd{U}-T\dd{S} &= (\var{q}-T\dd{S})+\var{w}\\
            &\leq \var{w}
        \end{align*}
        \item But since $A=U-TS$ is clearly a state function (as a combination of state functions and state variables),
        \begin{equation*}
            \dd{A} = \dd{U}-T\dd{S}-S\dd{T}
        \end{equation*}
        is an exact differential for a state function. In particular, at constant temperature,
        \begin{equation*}
            \dd{A} = \dd{U}-T\dd{S}
        \end{equation*}
        \item Therefore, $\dd{A}\leq\var{w}$ at constant temperature.
        \item Thus, at constant $T,V,w$, $\dd{A}\leq 0$. In other words, $A$ is monotonically decreasing.
    \end{itemize}
    \item \textbf{Helmholtz free energy}: The state function $A$ defined above.
    \begin{itemize}
        \item We use this because it's more "natural" to think about finding the lowest free \emph{energy} state than the largest \emph{entropy} state.
        \item The Helmholtz free energy is most often used in physics where there isn't often a "pressure bath." Chemists prefer the \textbf{Gibbs free energy}, which is constructed exactly the same way.
    \end{itemize}
    \item \textbf{Gibbs free energy}: The state function describing the free energy of a system at constant pressure. \emph{Denoted by} $\bm{G}$. \emph{Given by}
    \begin{equation*}
        G = H-TS
    \end{equation*}
    \begin{itemize}
        \item It follows from our prior results that at constant temperature,
        \begin{align*}
            \dd{G} &= \dd{H}-T\dd{S}\\
            &= \dd{U}+\dd{(PV)}-T\dd{S}\\
            &= \var{q}+\var{w}+P\dd{V}+V\dd{P}-T\dd{S}\\
            &= \var{q}-P\dd{V}+\var{w_{\text{non-rev }PV}}+P\dd{V}+V\dd{P}-T\dd{S}\\
            &= \var{q}-T\dd{S}+\var{w_{\text{non-rev }PV}}+V\dd{P}\\
            &\leq 0+\var{w_{\text{non-rev }PV}}+V\dd{P}
        \end{align*}
        \item Thus, at constant $T,P$ and with no other $w$ done on the system, $\dd{G}\leq 0$.
        \item In a reversible process (constant $T,P$), $\dd{G}=\var{w_{\text{non-rev }PV}}$.
    \end{itemize}
    \item Note that $\var{w}$ is the work put into the system, so $\var{w}\geq\var{w_\text{rev}}=\dd{A}$.
    \item The system produces more work when operated reversibly.
    \item Gibbs free energy and electrochemical work: The case of hydrogen fuel cells vs. thermal engines.
    \begin{itemize}
        \item The hydrogen fuel cell is based off of the reaction
        \begin{equation*}
            \ce{H2 + 1/2O2 -> H2O}
        \end{equation*}
        with $\Delta H=\SI[per-mode=symbol]{285.76}{\kilo\joule\per\mole}$ and $\Delta G=\SI[per-mode=symbol]{237.1}{\kilo\joule\per\mole}$.
        \item $\Delta H$ can be used to produce heat $q=\Delta H$.
        \item If we want to produce work, we have to run a thermal engine that will have efficiency bounded above by the Carnot cycle's $(T_h-T_c)/T_h$.
        \begin{itemize}
            \item With new materials, people have been able to use hotter thermal reservoirs and achieve efficiencies up to 50\%!
        \end{itemize}
        \item $\Delta G$ can be converged to work directly (as current and voltage in the fuel cell). Thus, in theory, runnning a reversible fuel cell produces more work than burning \ce{H2} in a reversible engine.
        \begin{itemize}
            \item The limitation is overpotential, though, and we would need a better hydrogen burning catalyst.
        \end{itemize}
    \end{itemize}
    \item Maxwell relations:
    \begin{equation*}
        \pdv{f}{x}{y} = \pdv{f}{y}{x}
    \end{equation*}
    \item If we want to show that $(\pdv*{P}{T})_V=(\pdv*{S}{V})_T$, we take
    \begin{align*}
        \dd{A} &= \var{q_\text{rev}}-T\dd{S}-S\dd{T}-P\dd{V}\\
        &= -S\dd{T}-P\dd{V}\\
        &= \left( \pdv{A}{T} \right)_V+\left( \pdv{A}{V} \right)_T
    \end{align*}
    so that
    \begin{equation*}
        \pdv{P}{T} = \pdv{T}(-\pdv{A}{V})
        = -\pdv{A}{T}{V}
        = -\pdv{A}{V}{T}
        = \pdv{V}(-\pdv{A}{T})
        = \pdv{S}{V}
    \end{equation*}
    \begin{itemize}
        \item Facts like these help us show things such as
        \begin{equation*}
            \Delta S = \int_{V_1}^{V_2}\pdv{S}{V}\dd{V}
            = \int_{V_1}^{V_2}\pdv{P}{T}\dd{V}
            = \int_{V_1}^{V_2}\frac{nR}{V}\dd{V}
            = nR\ln\frac{V_2}{V_1}
        \end{equation*}
        so we don't need to measure the heat flow to measure entropy when we have a more complex equation of state than $PV=nRT$.
    \end{itemize}
\end{itemize}



\section{Differential Relations}
\begin{itemize}
    \item \marginnote{2/11:}A useful equation for the change in energy per unit volume at constant temperature, and equation of state.
    \begin{itemize}
        \item We have that
        \begin{equation*}
            \dd{U} = \left( \pdv{U}{V} \right)_T\dd{V}+\left( \pdv{U}{T} \right)_V\dd{T}
        \end{equation*}
        \item It follows that
        \begin{align*}
            \dd{U} &= T\dd{S}-P\dd{V}\\
            &= T\left( \pdv{S}{T} \right)_V\dd{T}+\left( \pdv{S}{V} \right)_T\dd{V}-P\dd{V}\\
            &= \underbrace{T\left( \pdv{S}{T} \right)_V}_{(\pdv*{U}{T})_V}\dd{T}+\underbrace{\left[ \left( \pdv{S}{V} \right)_T-P \right]}_{(\pdv*{U}{V})_T}\dd{V}
        \end{align*}
        \item Thus, by Maxwell relations,
        \begin{align*}
            \left( \pdv{U}{V} \right)_T &= T\left( \pdv{S}{V} \right)_T-P\\
            &= T\left( \pdv{P}{T} \right)_V-P
        \end{align*}
        \item This equation can be evaluated given the equation of state.
        \item For an ideal gas, we get
        \begin{equation*}
            \left( \pdv{U}{V} \right)_T = T\frac{nR}{V}-P = 0
        \end{equation*}
        \item For a van der Waals gas (just the excluded volume $b$ part, not the $a$ term), we get
        \begin{equation*}
            \left( \pdv{U}{V} \right)_T = T\pdv{T}(\frac{nRT}{V-nb})-P
            = \frac{nRT}{V-nb}-P
            = 0
        \end{equation*}
        \item Now for small enough volume, we know that a gas will compress into a liquid. In other words, $T(\pdv{P}{T})_V\geq P$ always, where equality is only a good approximation at sufficiently large volumes.
    \end{itemize}
    \item A similar method can be applied to enthalpy changes with pressure.
    \begin{itemize}
        \item We can derive in an analogous method that
        \begin{equation*}
            \left( \pdv{H}{P} \right)_T = V+T\left( \pdv{S}{P} \right)_T
        \end{equation*}
    \end{itemize}
    \item You can only apply the chain rule to partial derivatives held constant with respect to the same variable(s).
    \item \textbf{Natural variables} (of a state function): A set of state variables for which the partial derivatives of the state function with respect to said variables are (simple expressions of) state variables or state functions.
    \begin{itemize}
        \item For example, $S,P$ are the natural variables of $U$ since $\pdv*{U}{S}=T$ and $\pdv*{U}{V}=-P$ as shown by
        \begin{equation*}
            \dd{U} = T\dd{S}-P\dd{V}
        \end{equation*}
        \item $V,T$ are not natural variables of $U$, as we showed earlier, since the partial derivatives of $U$ with respect to them are complicated expressions.
        \item For the other state functions, we have
        \begin{align*}
            \dd{H} &= T\dd{S}+V\dd{P}&
                \dd{A} &= -S\dd{T}-P\dd{V}\\
            \dd{S} &= \frac{1}{T}\dd{U}+\frac{P}{T}\dd{V}&
                \dd{G} &= -S\dd{T}+V\dd{P}
        \end{align*}
        \item Note that natural variables are not unique, as we also have
        \begin{equation*}
            \dd{S} = \frac{1}{T}\dd{H}-\frac{V}{T}\dd{P}
        \end{equation*}
    \end{itemize}
    \item An example of using Maxwell relations to getting $C_V$ to $C_P$ (19-27 and 22-11).
    \begin{itemize}
        \item We have that
        \begin{equation*}
            C_P-C_V = T\left( \pdv{P}{T} \right)_V\left( \pdv{V}{T} \right)_P
        \end{equation*}
        \item We also have that
        \begin{equation*}
            \dd{V} = \left( \pdv{V}{T} \right)_P\dd{T}+\left( \pdv{V}{P} \right)_T\dd{P}
        \end{equation*}
        \item It follows if we let $\dd{V}=0$ in the above expression that
        \begin{equation*}
            \left( \pdv{P}{T} \right)_V = -\frac{(\pdv*{V}{T})_P}{(\pdv*{V}{P})_T}
        \end{equation*}
        \item Therefore,
        \begin{align*}
            C_P-C_V &= T\left[ -\frac{(\pdv*{V}{T})_P}{(\pdv*{V}{P})_T} \right]\left( \pdv{V}{T} \right)_P\\
            &= -T\frac{(\pdv*{V}{T})_P^2}{(\pdv*{V}{P})_T}\\
            &= -T\frac{V^2\alpha^2}{-V\kappa}\\
            &= \frac{TV\alpha^2}{\kappa}
        \end{align*}
        where $\alpha$ is the \textbf{thermal expansion coefficient} and $\kappa$ is the \textbf{isothermal compressibility}.
    \end{itemize}
    \item \textbf{Thermal expansion coefficient}: The following constant. \emph{Denoted by} $\bm{\alpha}$. \emph{Given by}
    \begin{equation*}
        \alpha = \frac{1}{V}\left( \pdv{V}{T} \right)_P
    \end{equation*}
    \item \textbf{Isothermal compressibility}: The following constant. \emph{Denoted by} $\bm{\kappa}$. \emph{Given by}
    \begin{equation*}
        \kappa = -\frac{1}{V}\left( \pdv{V}{P} \right)_T
    \end{equation*}
\end{itemize}



\section{Office Hours (PGS)}
\begin{itemize}
    \item So is the content of the midterm Chapters 17-22 plus parts of Chapter 25 (the Maxwell-Boltzmann distribution)?
    \item Are there going to be any surprizes I should look out for (a la surface tension)?
    \item Will questions be like homework questions, quiz questions?
    \item Are we going to have to know derivations or just formulas?
\end{itemize}



\section{Chapter 21: Entropy and the Third Law of Thermodynamics}
\emph{From \textcite{bib:McQuarrieSimon}.}
\begin{itemize}
    \item \marginnote{2/3:}In this chapter, we will learn how to calculate absolute (as opposed to relative) values of entropy.
    \item Relating thermodynamic quantities to entropy.
    \begin{itemize}
        \item It follows from the First Law of Thermodynamics that
        \begin{equation*}
            \dd{U} = \underbrace{T\dd{S}}_{\var{q_\text{rev}}}\underbrace{-P\dd{V}}_{\var{w_\text{rev}}}
        \end{equation*}
        \item We have that the total differential of $U(V,T)$ is
        \begin{equation*}
            \dd{U} = \pdv{U}{T}\dd{T}+\pdv{U}{V}\dd{V}
        \end{equation*}
        \item Combining the two above equations, we have that
        \begin{align*}
            T\dd{S}-P\dd{V} &= \pdv{U}{T}\dd{T}+\pdv{U}{V}\dd{V}\\
            \dd{S} &= \frac{1}{T}\pdv{U}{T}\dd{T}+\frac{1}{T}\left( P+\pdv{U}{V} \right)\dd{V}\\
            &= \frac{C_V\dd{T}}{T}+\frac{1}{T}\left( P+\pdv{U}{V} \right)\dd{V}
        \end{align*}
        \item It follows by comparing the above with the total differential of $S(V,T)$ that
        \begin{align*}
            \left( \pdv{S}{T} \right)_V &= \frac{C_V}{T}&
            \left( \pdv{S}{V} \right)_T &= \frac{1}{T}\left[ P+\left( \pdv{U}{V} \right)_T \right]
        \end{align*}
    \end{itemize}
    \item Since
    \begin{equation*}
        \dd{H} = \dd{U}+P\dd{V}+V\dd{P}
        = T\dd{S}+V\dd{P}
    \end{equation*}
    we can proceed in a similar manner to the above to obtain
    \begin{align*}
        \left( \pdv{S}{T} \right)_P &= \frac{C_P}{T}&
        \left( \pdv{S}{P} \right)_T &= \frac{1}{T}\left[ \left( \pdv{H}{P} \right)-V \right]
    \end{align*}
    \item \textbf{Third Law of Thermodynamics}: Every substance has a finite positive entropy, but at zero kelvin, the entropy may become zero, and does so in the case of a perfectly crystalline substance.
    \begin{itemize}
        \item While the first and second laws provide state functions (internal energy and entropy, respectively), the third law provides a numerical scale for entropy.
    \end{itemize}
    \item The third law, although formulated before quantum mechanics, follows nicely from it: At absolute zero, every system in an ensemble will be in the same energy state, so $W=1$; it follows that $S=k_B\ln 1=0$.
    \begin{itemize}
        \item If, however, the ground state has a degeneracy of $n$, then
        \begin{align*}
            S(\SI{0}{\kelvin}) &= -k_B\sum_jp_j\ln p_j\\
            &= -k_B\sum_{j=1}^n\frac{1}{n}\ln\frac{1}{n}\\
            &= k_B\ln n
        \end{align*}
        \item Nevertheless, even for a degeneracy of $N_A$, we will only have $S(\SI{0}{\kelvin})\approx\SI{7.56e-22}{\joule\per\mole\per\kelvin}$, which is far less than any measurable value.
    \end{itemize}
    \item Because of the third law, we can define entropy absolutely (assuming no phase change between 0 and $T$).
    \begin{align*}
        S(T) &= S(0)+S(T)-S(0)\\
        &= S(0)+\Delta S\\
        &= S(0)+\int_0^T\frac{C_P(t)}{t}\dd{t}\\
        &= \int_0^T\frac{C_P(t)}{t}\dd{t}
    \end{align*}
    \item Accounting for phase changes.
    \begin{itemize}
        \item A phase change is a great example of a reversible process since we only need the temperature to be slightly above or slightly below the transition temperature $T_\text{trs}$ to accomplish it.
        \item Thus,
        \begin{equation*}
            \Delta_\text{trs}S = \frac{q_\text{rev}}{T_\text{trs}}
            = \frac{\Delta_\text{trs}H}{T_\text{trs}}
        \end{equation*}
        \item It follows that, as applicable,
        \begin{equation*}
            S(T) = \int_0^{T_\text{fus}}\frac{C_P^s(t)}{t}\dd{t}+\frac{\Delta_\text{fus}H}{T_\text{fus}}+\int_{T_\text{fus}}^{T_\text{vap}}\frac{C_P^l(t)}{t}\dd{t}+\frac{\Delta_\text{vap}H}{T_\text{vap}}+\int_{T_\text{vap}}^T\frac{C_P^g(t)}{t}\dd{t}
        \end{equation*}
        \item Note that with typical values plugged in, $\Delta_\text{fus}S\ll\Delta_\text{vap}S$.
    \end{itemize}
    \item \textbf{Debye $\bm{T^3}$ law}: As $T\to 0$ (i.e., for about $T=\SIrange{0}{15}{\kelvin}$), $C_P^s(T)\to T^3$ for most nonmetallic crystals and $C_P^s(T)\to aT+bT^3$ ($a,b\in\R_{\geq 0}$) for most metallic crystals.
    \begin{itemize}
        \item It follows by the absolute definition of $S$ that
        \begin{equation*}
            S(T) = \frac{C_P(T)}{3}
        \end{equation*}
        at low temperatures for nonmetallic solids.
    \end{itemize}
    \item \textbf{Debye temperature}: A constant characteristic of the solid. \emph{Denoted by} $\bm{\Theta_\textbf{D}}$.
    \item \textbf{Third-law entropy}: An absolute entropy value calculated according to the convention that $S(\SI{0}{\kelvin})=0$. \emph{Also known as} \textbf{practical absolute entropy}.
    \item \textcite{bib:McQuarrieSimon} calculates the third-law entropy of \ce{N2} based on various thermodynamic data.
    \item \textbf{Standard entropy}: An entropy value of a gas as presented in the literature. \emph{Denoted by} $\bm{S^\circ}$. \emph{Units} $\bm{\textbf{J}\,\textbf{mol}^{-1}\,\textbf{K}^{-1}}$.
    \begin{itemize}
        \item Standard entropies are, by convention, corrected for the nonideality of the gas at one bar (for how to calculate this correction, see Chapter 22).
    \end{itemize}
    \item \textcite{bib:McQuarrieSimon} rederives $S(\SI{0}{\kelvin})=k_B\ln n$ from a partition function approach, and then gives a specific example for \ce{N2}, noting how well this value correlates with the one from the previous example.
    \begin{itemize}
        \item "This type of agreement is quite common, and in many cases the statistical thermodynamic value is more accurate than the calorimetric value\dots The accepted literature values are often a combination of statistical thermodynamic and calorimetric values" \parencite[863]{bib:McQuarrieSimon}.
        \item Also gives a linear symmetric example (\ce{CO2}).
    \end{itemize}
    \item Entropy trends.
    \begin{enumerate}
        \item "The standard molar entropies of the gaseous substances are the largest, and the standard molar entropies of the solid substances are the smallest" \parencite[865]{bib:McQuarrieSimon}.
        \item "The increase in standard molar entropy of the noble gases is a consequence of their increasing mass as we move down the periodic table" \parencite[865]{bib:McQuarrieSimon}.
        \begin{itemize}
            \item More mass implies more translational energy levels are available implies more disorder.
            \item This is a consequence of quantum mechanics --- considering the formula for the energy of a particle in a 3D box, note that $m$ is in the denominator. Thus, increasing $m$ means that the levels are more closely spaced, and hence more are readily accessible.
        \end{itemize}
        \item "Generally speaking, the more atoms of a given type in a molecule, the greater is the capacity of the molecule to take up energy and thus the greater is its entropy" \parencite[866]{bib:McQuarrieSimon}.
        \begin{itemize}
            \item More atoms implies more vibrational modes implies more disorder.
        \end{itemize}
        \item "For molecules with approximately the same molecular masses, the more compact the molecule is, the smaller is its entropy" \parencite[867]{bib:McQuarrieSimon}.
        \begin{itemize}
            \item Between isomers, the one with more unrestricted motion will have greater entropy.
        \end{itemize}
    \end{enumerate}
    \item We can calculate the entropy for substances that "don't exist" via alternate paths.
    \begin{itemize}
        \item For example, \ce{Br2} is a liquid at \SI{298}{\kelvin}, but we can still calculate $S^\circ[\ce{Br2_{(g)}}]$ by imagining the following path: Raise \ce{Br2_{(l)}} to its boiling point; vaporize it; cool it back down to \SI{298}{\kelvin}.
        \item This calculated value is in agreement with the spectroscopic value.
    \end{itemize}
    \item \textbf{Residual entropy}: The difference between the calculated molar entropy of a substance and its experimental molar entropy. \emph{Given by}
    \begin{equation*}
        \overline{S}_\text{calc}-\overline{S}_\text{exp}
    \end{equation*}
    \begin{itemize}
        \item We define residual entropy this way because wherever there are discrepancies, it so happens that $S_\text{calc}>S_\text{exp}$.
    \end{itemize}
    \item Large residual entropies are encountered for the linear molecules \ce{CO} and \ce{N2O}.
    \begin{itemize}
        \item This is because these molecules have small dipole moments, so upon crystallization, there is not a strong tendency for the molecule to align in the most energetically favorable way.
        \item Thus, with molecules already locked in higher energy states at $T_\text{fus}$, as we cool to \SI{0}{\kelvin}, we do not have a "perfect" crystal.
        \item Since the ground state is two-fold degenerate in both of these cases (we have \ce{CO} and \ce{OC}, and \ce{NNO} and \ce{ONN}), $\overline{S}(\SI{0}{\kelvin})=R\ln 2$ here.
    \end{itemize}
    \item We can similarly account for the larger still residual entropy in \ce{H3CD} by noting that the ground state is four-fold degenerate, and thus $\overline{S}(\SI{0}{\kelvin})=R\ln 4$ here.
    \item We can use standard molar entropies to calculate the entropy changes of chemical reactions.
\end{itemize}




\end{document}